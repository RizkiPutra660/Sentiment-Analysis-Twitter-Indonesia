{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Tweets Indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rizki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "c:\\Users\\rizki\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "data = pd.read_csv('tweets_662k.csv', encoding =DATASET_ENCODING, dtype={'text': \"string\"})\n",
    "data['text']=data['text'].apply(str)\n",
    "X = data.iloc[:,[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-preprocessing\n",
    "\n",
    "# Missing Values\n",
    "num_missing_desc = data.isnull().sum()[2]    # No. of values with missing descriptions\n",
    "print('Number of missing values: ' + str(num_missing_desc))\n",
    "data = data.dropna()\n",
    "\n",
    "TAG_CLEANING_RE = \"@\\S+\"\n",
    "# Remove @tags\n",
    "X['text'] = X['text'].map(lambda x: re.sub(TAG_CLEANING_RE, ' ', x))\n",
    "\n",
    "# Smart lowercase\n",
    "X['text'] = X['text'].map(lambda x: x.lower())\n",
    "\n",
    "# Remove numbers\n",
    "X['text'] = X['text'].map(lambda x: re.sub(r'\\d+', ' ', x))\n",
    "\n",
    "# Remove links\n",
    "TEXT_CLEANING_RE = \"https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "X['text'] = X['text'].map(lambda x: re.sub(TEXT_CLEANING_RE, ' ', x))\n",
    "\n",
    "# Remove Punctuation\n",
    "X['text']  = X['text'].map(lambda x: x.translate(x.maketrans('', '', string.punctuation)))\n",
    "\n",
    "# Remove white spaces\n",
    "X['text'] = X['text'].map(lambda x: x.strip())\n",
    "\n",
    "# Tokenize into words\n",
    "X['text'] = X['text'].map(lambda x: word_tokenize(x))\n",
    " \n",
    "# Remove non alphabetic tokens\n",
    "X['text'] = X['text'].map(lambda x: [word for word in x if word.isalpha()])\n",
    "\n",
    "# Filter out stop words\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "X['text'] = X['text'].map(lambda x: [w for w in x if not w in stop_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# stemmed\n",
    "def stemmed_wrapper(term):\n",
    "    return stemmer.stem(term)\n",
    "\n",
    "term_dict = {}\n",
    "\n",
    "for document in X['text']:\n",
    "    for term in document:\n",
    "        if term not in term_dict:\n",
    "            term_dict[term] = ' '\n",
    "            \n",
    "print(len(term_dict))\n",
    "print(\"------------------------\")\n",
    "\n",
    "for term in term_dict:\n",
    "    term_dict[term] = stemmed_wrapper(term)\n",
    "    print(term,\":\" ,term_dict[term])\n",
    "    \n",
    "print(term_dict)\n",
    "print(\"------------------------\")\n",
    "\n",
    "\n",
    "# apply stemmed term to dataframe\n",
    "def get_stemmed_term(document):\n",
    "    return [term_dict[term] for term in document]\n",
    "\n",
    "X['text'] = X['text'].swifter.apply(get_stemmed_term)\n",
    "\n",
    "# Turn lists back to string\n",
    "X['text'] = X['text'].map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv(\"Clean_Tweets_Indonesia.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
